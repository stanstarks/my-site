<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>My Hakyll Blog - Music Visualization</title>
        <link rel="stylesheet" type="text/css" href="../css/syntax.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">My Hakyll Blog</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
				<a href="../cv.html">CV</a>
                <a href="../archive.html">Archive</a>
				<a href="../etcetera.html">ETC</a>
            </div>
        </div>

        <div id="content">
            <h1>Music Visualization</h1>

            <div class="info">Posted on May 31, 2013</div>

<p>Very long ago, I implemented the <a href="https://en.wikipedia.org/wiki/Cocktail_party_effect">cocktail party problem</a> with PCA for a Multivariate Analysis assignment. Even though my teacher thought PCA should be used for dimension reduction and gave me a very low score, I am amazed about how music can be played as data.</p>
<p>Later I learned how MP3 compresses songs and got some basic knowledge about NLP. I never stopped dreaming about interpreting music with machine ever since. Or more specifically, I’d like my computer to write guitar pro tabs for me or remove one of the instruments so that I can stub in as any member in a band.</p>
<p>Recently I finally decide to read some papers on this subject and found out it is called <a href="https://en.wikipedia.org/wiki/Music_information_retrieval">Music information retrieval</a> (MIR) and people are making money with it.</p>
<p>Sound is characterized by pitch, loudness and timbre. And my dream program must be able to split the sound by instrument and classify them by timbre. <a href="http://infoscience.epfl.ch/record/33874/files/DePoliP97.pdf" title="De Poli, G., Prandoni, P.: Sonological models for timbre characterization. Journal of New Music Research 26, 170–197 (1997)">De Poli and Prandoni</a> used a 6 MFCC coefficients vector as input to a Kohonen self-organizing map (SOM) in order to build timbre spaces. <a href="http://asp.eurasipjournals.com/content/pdf/1687-6180-2003-943279.pdf" title="Agostini, G., Longari, M., Pollastri, E.: Musical instrument timbres classification with spectral features. EURASIP Journal on Applied Signal Processing, 5–14 (2003)">Agostini et al.</a> employed only spectral characteristics of 1007 notes from 27 musical instruments classified with support vector machines and quadratic discriminate analysis. The most relevant features were to be the in harmonicity, the spectral centroid, and the energy contained in the first partial.</p>
<p>Some researchers use this kind of features to classify single notes with classifiers like <span class="math"><em>k</em></span>-NN or GMM. Some of the results are very recent. However, this is quite far from what I am expecting. First of all, I want it to be unsupervised. Vocals and instruments (especially electric ones) can differ very much. I would like to separate them rather than label them.</p>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
