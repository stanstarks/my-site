<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>My Hakyll Blog - Some thoughts about Deep Learning</title>
        <link rel="stylesheet" type="text/css" href="../css/syntax.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">My Hakyll Blog</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
				<a href="../cv.html">CV</a>
                <a href="../archive.html">Archive</a>
				<a href="../etcetera.html">ETC</a>
            </div>
        </div>

        <div id="content">
            <h1>Some thoughts about Deep Learning</h1>

            <div class="info">Posted on May 31, 2013</div>

<p>Training RBMs ————  RBMs are a special case of Boltzman machines and Markov random fields. Their graphical model corresponds to that of factor analysis.  RBM’s applications: ————  [<em>Dimension Reduction</em>][1]  In this Science paper, a RBM is used to model images where pixels are visible units. A joint configuration <span class="math">(<strong>v</strong>, <strong>h</strong>)</span> of the visible and hidden units has an [energy][2] given by  <br /><span class="math"><em>E</em>(<strong>v</strong>, <strong>h</strong>) =  − ∑ <sub><em>i</em> ∈ pixels</sub><em>b</em><sub><em>i</em></sub><em>v</em><sub><em>i</em></sub> − ∑ <sub><em>j</em> ∈ features</sub><em>b</em><sub><em>j</em></sub><em>h</em><sub><em>j</em></sub> − ∑ <sub><em>i</em>, <em>j</em></sub><em>v</em><sub><em>i</em></sub><em>h</em><sub><em>j</em></sub><em>w</em><sub><em>i</em><em>j</em></sub></span><br />  where <span class="math"><em>v</em><sub><em>i</em></sub></span> and <span class="math"><em>h</em><sub><em>j</em></sub></span> are the binary states of pixel <span class="math"><em>i</em></span> and feature <span class="math"><em>j</em></span>, <span class="math"><em>b</em><sub><em>i</em></sub></span> and <span class="math"><em>b</em><sub><em>j</em></sub></span> are their biases, and <span class="math"><em>w</em><sub><em>i</em><em>j</em></sub></span> is the weight between them. The probability that the model assigns to a visible vector, <span class="math"><strong>v</strong></span> is  <br /><span class="math">$p(\mathrm{v}) = \sum_{\mathbf{h}\in\mathcal{H}}p(\mathbf{v}, \mathbf{h})=\frac{\sum_h\exp(-E(\mathbf{v},\mathbf{h}))}{\sum_{\mathbf{u}, \mathbf{g}}\exp(-E(\mathbf{u},\mathbf{g}))}$</span><br />  where <span class="math">H</span> is the set of all possible binary hidden vectors. When using linear visible units and binary hidden units, the energy function and update rules are particularly simple if the linear units have Gaussian noise with unit variance. If the variances are not <span class="math">1</span>, the energy function becomes:  <br /><span class="math">$E(\mathbf{v}, \mathbf{h}) = -\sum_{i\in\mathrm{pixels}}\frac{(v_i-b_i)^2}{2\sigma^2_i} - \sum_{j\in\mathrm{features}}b_jh_j-\sum_{i,j}\frac{v_i}{\sigma_i}h_jw_{ij}$</span><br />  where <span class="math"><em>σ</em><sub><em>i</em></sub></span> is the standard deviation of the Gaussian noise for visible unit <span class="math"><em>i</em></span>. The stochastic update rule for the hidden units remains the same except that each <span class="math"><em>v</em><sub><em>i</em></sub></span> is divided by <span class="math"><em>σ</em><sub><em>i</em></sub></span>. The update rule for visible units <span class="math"><em>i</em></span> is to sample from a Gaussian with mean <span class="math"><em>b</em><sub><em>i</em></sub> + <em>σ</em><sub><em>i</em></sub>∑ <sub><em>j</em></sub><em>h</em><sub><em>j</em></sub><em>w</em><sub><em>i</em><em>j</em></sub></span> and variance <span class="math"><em>σ</em><sub><em>i</em></sub><sup>2</sup></span>.  The best thing about RBM is that the layer-by-layer learning can be repeated as many times as desired.  [<em>Classifcation</em>][3]  [1]: http://www.cs.toronto.edu/~hinton/science.pdf “Geoffrey Hinton and Ruslan Salakhutdinov (2006). Reducing the dimensionality of data with neural networks. Science 28:504–507.”  [2]: http://www.pnas.org/content/79/8/2554.full.pdf “J. J. Hopfield, Proc. Natl. Acad. Sci. U.S.A. 79, 2554 (1982).”  [3]: http://machinelearning.org/archive/icml2008/papers/601.pdf “Hugo Larochelle and Yoshua Bengio (2008). Classification using discriminative restricted Boltzmann machines. Proc. 25th Int’l Conf. on Machine Learning.”</p>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
